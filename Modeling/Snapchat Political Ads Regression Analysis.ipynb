{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapchat Political Ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "Using the Snapchat Political Ads Data from 2018 and 2019, we are attempting to predict the number of Impressions on a given ad. This prediction would tell us how many people would see a given ad. Since Impressions is quantitative, this problem is a regression problem. The evaluation metric we used for our models is R-Squared, a measure of variance in the target that is explained by our features. A higher R-Squared means that the model is more accurate.\n",
    "\n",
    "### Baseline Model\n",
    "Our baseline model was based only on information that we would know at the time of prediction or when the ad was launched. This means that we could not use the columns Spend or Impressions as they are not determined until the ad campaign ends. We decided to use 4 features, Currency Code, Gender, Country Code, and Language. We chose them because they had a limited number of unique values. These were all nominal features so we one-hot encoded them. Having a limited number of unique values was important because it ensured that the column transformations were less likely to result in a sparse dataset and the model would not produce unexpected results. \n",
    "\n",
    "Our baseline model used linear regression. We measured the performance of our model using R^2. We generated multiple training and testing sets from the ads dataset, calculated R^2 and then took the average of the scores. We got a mean R^2 of -0.0038. This model has poor performance which we attributed to the fact that none of our features reveal meaningful information about the reach of an add. To develop a better model, we added several engineered features.  \n",
    "\n",
    "\n",
    "### Final Model\n",
    "The first feature we added was improving the One Hot Encoding of the nominal features from the base model by adding a PCA transformer. This is good for our data because the dimensionality reduction will eliminate unnecessary features created from the One Hot Encoding done right before. We chose the parameters for PCA through a grid search to maximize our R-Squared value.\n",
    "\n",
    "The second feature we added was calculating the duration of time the ad was up on Snapchat for. We thought this information could be valuable because the length of an ad could indicate how many people saw the ad. If the ad was up for longer, then more people had a chance to see it. We made this feature by finding the difference between the specified start and end date. The ads with no specified end date are still running according to Snapchat, so we replaced the null entries in that column with the current date for calculating duration.\n",
    "\n",
    "The rest of the features we added were transformations of nominal columns that werenâ€™t previously One Hot Encoded into quantitative measures of the character length of the entries in each row. The columns that we performed this transformation on were all related to location data that determined where the app would run. This means that the more localized the ad is, the longer the entries in those columns would be. This transformation helps our data because if an ad is highly localized, it would most likely get less impressions since the number of people it is shown to is limited.\n",
    "\n",
    "We tested our features with linear and logistic regression. We ultimately chose linear regression for our model because logistic regression took a long time to run and was giving worse results. For our final model with linear regression, we generated multiple training and testing sets from the ads dataset, calculated R^2 and then took the average of the scores. It had an average R-Squared score of 0.006728 over 100 runs. This model still has pretty poor performance overall but is an improvement from our baseline model. \n",
    "\n",
    "\n",
    "### Fairness Evaluation\n",
    "The subsets of the data that we wanted to evaluate fairness on were ads with lower impressions (<100,000 views) and ads with higher impressions (>= 100,000 views). The parity measure that we used was the difference of R-Squared scores since the model is solving a regression problem. \n",
    "\n",
    "- Our null hypothesis was that the R-Squared values of the lower and higher impressions classes are approximately equal.\n",
    "\n",
    "- Our alternate hypothesis was that the R-Squared values of the lower and higher impressions classes are not approximately equal.\n",
    "\n",
    "Using a permutation test, we found that our p-value for the test was 0.0, averaged over 100 runs. The cause of this was the high difference of R-Squared scores on the observed data, particularly from the larger negative score on the lower impressions subset. This tells us that our model is unfair, and is more likely to score well on ads with higher impressions. This was a surprise to us, as we would have guessed that the engineered features would be biased towards more localized, lower viewed ads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data sets\n",
    "ads_2018 = os.path.join('PoliticalAds2018.csv')\n",
    "ads18 = pd.read_csv(ads_2018)\n",
    "ads_2019 = os.path.join('PoliticalAds2019.csv')\n",
    "ads19 = pd.read_csv(ads_2019)\n",
    "\n",
    "# concat the data sets\n",
    "ads = pd.concat([ads18, ads19], ignore_index=True)\n",
    "\n",
    "# Data Cleaning\n",
    "# convert Start and End dates to date time objects + to PST\n",
    "ads['StartDate'] = pd.to_datetime(ads['StartDate']).dt.tz_convert(tz=\"America/Los_Angeles\")\n",
    "ads['EndDate'] = pd.to_datetime(ads['EndDate']).dt.tz_convert(tz=\"America/Los_Angeles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.003809749261793306"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting appropriate features for baseline model\n",
    "features = ['Currency Code', 'Gender', 'CountryCode', 'Language']\n",
    "baseline_feats = ads[features].fillna('Missing')\n",
    "\n",
    "# features\n",
    "X = baseline_feats\n",
    "# outcome\n",
    "y = ads.Impressions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "base_pl = Pipeline([('one-hot', OneHotEncoder(handle_unknown = 'ignore')), ('lin-reg', LinearRegression())])\n",
    "\n",
    "#finding the average R^2 score over 100 iterations \n",
    "N = 100\n",
    "out = []\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "    base_pl.fit(X_train, y_train)\n",
    "    out.append(base_pl.score(X_test, y_test))\n",
    "\n",
    "base_r2 = np.mean(out)\n",
    "base_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting features and filling missing values for one hot encoding\n",
    "features = ['Currency Code', 'Gender', 'CountryCode', 'Language']\n",
    "final_feats = ads[features].fillna('Missing')\n",
    "final_feats[['EndDate', 'StartDate', 'Metros (Included)', \n",
    "             'Metros (Excluded)', 'Radius Targeting (Included)', \n",
    "             'Radius Targeting (Excluded)', 'Postal Codes (Included)', \n",
    "             'Postal Codes (Excluded)']] = ads[['EndDate', 'StartDate', 'Metros (Included)', 'Metros (Excluded)',\n",
    "                                                'Radius Targeting (Included)', 'Radius Targeting (Excluded)', \n",
    "                                                'Postal Codes (Included)', 'Postal Codes (Excluded)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to apply transformations by row \n",
    "def by_col(col, func):\n",
    "    r_s = col.apply(func, axis=1)\n",
    "    return r_s.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st Feature\n",
    "## Engineer duration from StartDate and EndDate\n",
    "\n",
    "def calc_duration(row):\n",
    "    if pd.isnull(row['EndDate']):\n",
    "        return (pd.to_datetime('today').tz_localize(\"America/Los_Angeles\") - row['StartDate']).days\n",
    "    else:\n",
    "        return (row['EndDate'] - row['StartDate']).days\n",
    "\n",
    "duration_transform = Pipeline([('Duration', FunctionTransformer(by_col, kw_args={'func':calc_duration}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd Feature\n",
    "## Engineer Localized feature which estimates how 'localized' an ad is by counting the number of characters in the column\n",
    "\n",
    "def str_length(row):\n",
    "    if type(row.iloc[0]) == str:\n",
    "        return len(row.iloc[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "local_transform = Pipeline([('Localized', FunctionTransformer(by_col, kw_args={'func' :str_length}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006727598969040846"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble final pipeline with one hot encoding and engineered features\n",
    "one_hot_pl = Pipeline([('one-hot', OneHotEncoder(handle_unknown = 'ignore', sparse=False)),\n",
    "                       ('pca', PCA(svd_solver = 'full', n_components= 37))\n",
    "                      ])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[('Duration', duration_transform, ['StartDate', 'EndDate']),\n",
    "                                             ('LocalizationScore_mi', local_transform, ['Metros (Included)']),\n",
    "                                             ('LocalizationScore_me', local_transform, ['Metros (Excluded)']),\n",
    "                                             ('LocalizationScore_rti', local_transform, ['Radius Targeting (Included)']),\n",
    "                                             ('LocalizationScore_rte', local_transform, ['Radius Targeting (Excluded)']),\n",
    "                                             ('LocalizationScore_pci', local_transform, ['Postal Codes (Included)']),\n",
    "                                             ('LocalizationScore_pce', local_transform, ['Postal Codes (Excluded)']),\n",
    "                                             ('categorical', one_hot_pl, ['Currency Code','Gender', 'CountryCode', \n",
    "                                                                            'Language'])\n",
    "                                            ])\n",
    "\n",
    "final_pl = Pipeline(steps=[('preprocesser', preprocess), ('lin-reg', LinearRegression())])\n",
    "\n",
    "# final model performance \n",
    "# features\n",
    "X = final_feats\n",
    "# outcome\n",
    "y = ads.Impressions\n",
    "\n",
    "#finding the average R^2 score over 100 iterations \n",
    "N = 100\n",
    "out = []\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "    final_pl.fit(X_train, y_train)\n",
    "    out.append(final_pl.score(X_test, y_test))\n",
    "\n",
    "final_r2 = np.mean(out)\n",
    "final_r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stat(df):\n",
    "    #splitting data by less and more views and calculating R^2 score for each subset\n",
    "    more = df[df['Views'] == 1]\n",
    "    # features\n",
    "    X_more = more.drop(columns=['Impressions', 'Views'])\n",
    "    # outcome\n",
    "    y_more = more.Impressions\n",
    "    more_score = final_pl.score(X_more, y_more)\n",
    "    \n",
    "    less = df[df['Views'] == 0]\n",
    "    # features\n",
    "    X_less = less.drop(columns=['Impressions', 'Views'])\n",
    "    # outcome\n",
    "    y_less = less.Impressions\n",
    "    less_score = final_pl.score(X_less, y_less)\n",
    "    # test stat: difference of R^2 scores for ads with less views and ads with more views\n",
    "    return abs(more_score - less_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating model on fairness using a permutation test \n",
    "\n",
    "#selecting features and filling missing values for one hot encoding\n",
    "perm_data = ads[['EndDate', 'StartDate', 'Metros (Included)', 'Metros (Excluded)','Radius Targeting (Included)',\n",
    "                 'Radius Targeting (Excluded)', 'Postal Codes (Included)', 'Postal Codes (Excluded)', 'Impressions']]\n",
    "features = ['Currency Code', 'Gender', 'CountryCode', 'Language']\n",
    "perm_data[features] = ads[features].fillna('Missing')\n",
    "\n",
    "# adding in a binary column for ad Impressions with a threshold of 100,000 - 0 -> Less views, 1 -> More views \n",
    "perm_data['Views'] = (ads['Impressions'] >= 100000).astype(int)\n",
    "\n",
    "#putting together the Pipeline using one-hot encoding and engineered features\n",
    "one_hot_pl = Pipeline([('one-hot', OneHotEncoder(handle_unknown = 'ignore', sparse=False)),\n",
    "                       ('pca', PCA(svd_solver = 'full', n_components= 37))\n",
    "                      ])\n",
    "preprocess = ColumnTransformer(transformers=[('Duration', duration_transform, ['StartDate', 'EndDate']),\n",
    "                                             ('LocalizationScore_mi', local_transform, ['Metros (Included)']),\n",
    "                                             ('LocalizationScore_me', local_transform, ['Metros (Excluded)']),\n",
    "                                             ('LocalizationScore_rti', local_transform, ['Radius Targeting (Included)']),\n",
    "                                             ('LocalizationScore_rte', local_transform, ['Radius Targeting (Excluded)']),\n",
    "                                             ('LocalizationScore_pci', local_transform, ['Postal Codes (Included)']),\n",
    "                                             ('LocalizationScore_pce', local_transform, ['Postal Codes (Excluded)']),\n",
    "                                             ('categorical', one_hot_pl, ['Currency Code','Gender', 'CountryCode', \n",
    "                                                                            'Language'])\n",
    "                                            ])\n",
    "final_pl = Pipeline(steps=[('preprocesser', preprocess), ('lin-reg', LinearRegression())])\n",
    "#fitting the Pipeline to the data\n",
    "final_pl.fit(perm_data.drop(columns=['Impressions', 'Views']), perm_data.Impressions)\n",
    "\n",
    "# observed statistic\n",
    "observed = calc_stat(perm_data)\n",
    "\n",
    "# calculating test statistic on shuffled data \n",
    "N = 100\n",
    "results = []\n",
    "for _ in range(N):\n",
    "    # shuffle Views column\n",
    "    shuffled_views = (perm_data['Views'].sample(replace=False, frac=1).reset_index(drop=True))\n",
    "    shuffled = perm_data.assign(**{'Views': shuffled_views,})\n",
    "    results.append(calc_stat(shuffled))       \n",
    "    \n",
    "# calculate the p-value          \n",
    "p_val = np.count_nonzero(results >= observed) / N\n",
    "p_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
